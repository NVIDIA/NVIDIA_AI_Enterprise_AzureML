$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: nemo_lora
display_name: NeMo LoRA Fine-Tune
type: command
description: Component to run low-rank adaptation (LoRA) fine-tuning technique. <a href=https://github.com/NVIDIA/NVIDIA_AI_Enterprise_AzureML/blob/main/src/components/monai/3D_image_segmentation/train_segmentation/README.md>Reference file</a>
is_deterministic: true
version: 1
tags:
    # NVIDIA AI Enterprise: ""
    # Preview: ""
# distribution:
#   type: pytorch
# shm_size:
#   type: string
# # (input_data_dir, best_model_name, max_epochs):
inputs:
  customization_name:
    type: string
    description: a name for your model customization
  input_data_folder:
    type: uri_folder
    description: the input folder for training and test data files
  train_file:
    type: string
    description: the name of the .jsonl training data file
  val_file:
    type: string
    description: the name of the .jsonl val data file
  base_model:
    type: string
    description: pretrained NeMo GPT model used for customization
    default: GPT-5B
  batch_size:
    type: int
    description: the number of samples propogated concurrently through the network before the model parameters are updated in one iteration of training
    default: 8
  learning_rate:
    type: string
    description: how much to adjust the model parameters in response to the loss gradient
    default: "0.0001"
  orig_config_path:
    type: uri_folder
    description: location of the original config file for training
outputs:
  output_dir:
    type: uri_folder
    description: output directory for the checkpoints, config file, and model
  finetuned_model:
    type: uri_folder
    description: the LoRA fine-tuned model

code: ./src

command: >-
  python modify_config.py 
  --config-path ${{inputs.orig_config_path}} --model ${{inputs.base_model}} --input-data-folder ${{inputs.input_data_folder}}
  --train-file ${{inputs.train_file}} --val-file ${{inputs.val_file}} --output-dir ${{outputs.output_dir}} 
#python /NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py --config-path  ${{inputs.config_path}}
#python run.py $[[--epochs ${{inputs.max_epochs}}]] --initial_lr 0.00025 --train_batch_size 1 --val_batch_size 1 --input_data ${{inputs.input_data}} --best_model_name ${{inputs.best_model_name}} --model ${{outputs.model}}

environment: "azureml://registries/michxu-registry/environments/hwolff-bignlp-train/versions/1"
